# v1.3.2后的目标
- 更新$\lambda_r$的时候，如果每次都减半，这样的意义在哪
- 仔细分析加投影的问题，加在哪，如何加，为什么？
- 分析整个计算过程的阶的问题，考虑正交性能加速吗？

# v1.2.11
更改函数的使用方式。

# v1.2.10
初步实现了更新超梯度的功能，但是数值稳定性和效果有待提升。

# v1.2.8
使用了文章《MetaLearning_2017(477)_ICML34_Franceschi L_Forward and reverse gradient-based hyperparameter optimization.pdf》中的
向前传播计算超梯度的方法，但是暂时还未能验证超梯度的正确性。
由于本MVHSC中目标函数比较简单，且torch.autograd计算二阶导数的能力有限（也可能是我使用不太熟）。
目前使用的手动计算各种值，待能计算正确后进一步探索如何使用torch包计算二阶导数。

# 时间：2024年11月17日
对于$\delta_\phi$（超梯度）的计算问题，从v1.2.7开始尝试。
难点在于似乎还是没有完全理解计算超梯度的过程，同时反向传播的使用也不太熟练。

# 时间：2024年11月15日
内层的聚合梯度问题已经在 v1.2.6解决。

# 时间: 2024年11月12日
由于自动微分计算得到的梯度并未投影，所以计算结果总是错误。
该问题在 v1.2.3 之后被解决。

# 时间：2024年11月11日
由于可以使用自动微分求偏导，考虑使用双变量的函数解决问题。
该想法实践于 v1.2.0

# 时间：2024年11月6日
下层问题和上层问题的不同：
下层问题近似于一个正比例函数，无法在求最大值的同时使得梯度的范数也下降；（是否也意味着满足下层单元素条件？）
上层问题可以实现求最大值的同时，使得梯度的范数下降，也就是说是“上凸”的；

# 版本的命名准则
第二个数字的变更时，至少整个程序运行没有错误。

# 时间：2024年11月4日
使用get_data()时发现多次使用的get_data得到的标签不完全相同，大大影响了结果的稳定性。
如果在inner_loop或者outer_loop中不使用正交化，会导致结果溢出。

# 时间：2024年11月2日
基于结果的不稳定性，考虑添加一个变量用以保留最大值和最大值点。
单纯的改变目标函数的正负似乎没有任何影响。
添加最大值的变量后可以考虑记录达到最大值的时间。
考虑添加原来的投影映射到梯度中。








